{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the continuous action space lunar lander problem you can no longer use the DQN.\n",
    "The DQN outputs a Q value prediction for each action at each state.\n",
    "\n",
    "With a continuous action space you can no longer do this. \n",
    "\n",
    "Instead here is a policy gradient implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "Predict action given the input state\n",
    "\n",
    "Start an episode\n",
    "\n",
    "Use policy network with the state as input to predict the next action\n",
    "\n",
    "Take the action to get to the new state.\n",
    "\n",
    "Create a memory buffer of these transitions\n",
    "\n",
    "Have a critic network, calculate the q value for the state and action.\n",
    "Have a target critic network calculate the q value for the new state (use also the action at the new state decided by the actor_target network.\n",
    "\n",
    "Train the critic network based on the difference between these two. Update every so often. Temporal difference learning. \n",
    "\n",
    "Create the objective function - in this case you're trying to maximise the output of the critic\n",
    "\n",
    "Update the actor\n",
    "You just differentiate the critic wrt the actor (and go in the negative direction - gradient ascent)\n",
    "\n",
    "\n",
    "Best blog on pytorch and the fundamentals of backwards() and the graph \n",
    "https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n",
    "\n",
    "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63\n",
    "\n",
    "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b\n",
    "\n",
    "https://towardsdatascience.com/solving-lunar-lander-openaigym-reinforcement-learning-785675066197\n",
    "\n",
    "the total policy approaches summary\n",
    "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\n",
    "\n",
    "Good blog on DDPG\n",
    "https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    '''\n",
    "    This is a collection of transitions. (state, action reward sets)\n",
    "    inputs\n",
    "    size - defines the size of the storage.\n",
    "    memory - is the list that contains the transitions.\n",
    "    pointer - this identifies the current transition. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.memory = []\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def add_to_memory(self, transition):\n",
    "        if len(self.memory) == self.size:\n",
    "            self.memory[int(self.pointer)] = transition \n",
    "            self.pointer = (self.pointer + 1)% self.size\n",
    "        else:\n",
    "            self.memory.append(transition) \n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy network\n",
    "\n",
    "class policy_net(nn.Module):\n",
    "    '''\n",
    "    state_dim - dimension of the state\n",
    "    action_dim - dimension of the action (note this doesn't mean \n",
    "    number of discrete actions, it means number of continuous values that represent the action space)\n",
    "    max_action - if you need to clip the action to a certain range\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(policy_net, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        self.main= nn.Sequential(\n",
    "        nn.Linear(state_dim, 400),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.Linear(300, action_dim),\n",
    "        nn.Tanh()\n",
    "        # as tanh is between 0 and 1 if you multiply by max action you'll be between -max and +max action\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        output = output*self.max_action\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network\n",
    "\n",
    "class critic_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(critic_net, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 400),\n",
    "            nn.Linear(400, 300),\n",
    "            # for a combined action and state just output one Q value.\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        input_combined = torch.cat([input_state, input_action], 1)\n",
    "        output = self.main(input_combined)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = len(env.action_space.sample())\n",
    "state_dim = len(env.observation_space.sample())\n",
    "max_action = 1\n",
    "replay_buffer_size = 10000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy network\n",
    "actor = policy_net(state_dim, action_dim, max_action)\n",
    "actor_target = policy_net(state_dim, action_dim, max_action)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic = critic_net(state_dim, action_dim)\n",
    "critic_target = critic_net(state_dim, action_dim)\n",
    "\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "# Need to copy over the original parameters from critic and actor to the targets.\n",
    "\n",
    "#critic optimiser\n",
    "critic_optimiser = optim.Adam(critic.parameters(), lr=0.001)\n",
    "actor_optimiser = optim.Adam(actor.parameters(), lr = 0.001)\n",
    "\n",
    "# create the replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training \n",
    "\n",
    "episodes = 1\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # reset and get the first state\n",
    "    # state is a numpy array. mutable.\n",
    "    state = env.reset()\n",
    "    \n",
    "    for steps in range(max_steps): \n",
    "        # Predict the action\n",
    "        # Need to convert state to a tensor before going into network.\n",
    "        state_tensor = torch.tensor(state)\n",
    "\n",
    "        action = actor.forward(state_tensor)\n",
    "        # convert to numpy to put into the environment. \n",
    "        numpy_action = action.data.numpy()\n",
    "\n",
    "        # Find the next state.\n",
    "        next_state, reward, done, _ = env.step(numpy_action)\n",
    "\n",
    "        # store transitions into the replay buffer\n",
    "        transition = [state, action, reward, next_state]\n",
    "        replay_buffer.add_to_memory(transition)\n",
    "\n",
    "        # Move onto the next state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    \n",
    "    # Optimise networks: Once a episode?\n",
    "    \n",
    "    # sample from buffer\n",
    "    replay_buffer.sample(batch_size)\n",
    "    \n",
    "    #Critic\n",
    "    # Think about the critic model\n",
    "    # here we input the state and the action to get the q value\n",
    "    # state and action are already tensors. \n",
    "    q_value = critic.forward(state, action)\n",
    "    \n",
    "    # how to train the critic (use temporal difference?)\n",
    "    # what's the reward + q_value at next state\n",
    "    next_action = action_target(next_state)\n",
    "    q_value_next_state = critic_target(next_state, next_action)\n",
    "    y = reward + q_value_next_state\n",
    "    # critic_loss - difference between y and q_value. \n",
    "    # what type of loss metric to use. \n",
    "    critic_loss = mse_loss(q_value, y)\n",
    "    \n",
    "    critic_optimiser.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimiser.step()\n",
    "    \n",
    "    #Actor\n",
    "    # differentiate the critic with respect to the actor and ascend gradient.\n",
    "    # ?????\n",
    "    actor_loss = -q_value\n",
    "    actor_optimiser.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimiser.step()\n",
    "    \n",
    "    #Critic Target\n",
    "    # every x cycles update\n",
    "    # polyak averaging?\n",
    "    for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    #Actor Target\n",
    "    # every y cycles update\n",
    "    # polyak averaging?\n",
    "    for param, target_param in zip(actor.parameters(), critic_target.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    # Add in the ability to save a pretrain model. This might be useful when\n",
    "    # training and testing etc. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
