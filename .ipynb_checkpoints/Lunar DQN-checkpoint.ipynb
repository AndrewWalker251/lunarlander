{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from torch import optim\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning Network \n",
    "\n",
    "Using a deep Q network to solve the discrete lunar lander challenge. https://gym.openai.com/envs/LunarLander-v2/\n",
    "\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "\n",
    "Useful references:\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "As part of the implementation I would like to investigate a number of the hyperparmeters. \n",
    "\n",
    "- epochs\n",
    "\n",
    "- shape of the network \n",
    "     - number of layers\n",
    "     - no softmax at the end\n",
    "\n",
    "- learning rate \n",
    "\n",
    "- discount factor\n",
    "\n",
    "- replay buffer size\n",
    "\n",
    "- batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example action 2\n",
      "Example observation space [ 7.5206754e-04  1.3987600e+00  7.6157108e-02 -5.4044640e-01\n",
      " -8.6463278e-04 -1.7250784e-02  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# create the environment and explore the action and observation space.\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "print('Example action {}'.format(env.action_space.sample()))\n",
    "print('Example observation space {}'.format(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at what it looks like without any training.\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Deep Q Learner From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm actually going to build a double deep q network. This means I have two networks where the paramters from one are copied over to the other every so many iterations.\n",
    "\n",
    "There are 4 key elements.\n",
    "\n",
    " -reply memory \n",
    "\n",
    " -Q network\n",
    "\n",
    " -Target network\n",
    "\n",
    " -Training cycle. \n",
    " \n",
    " Bellman equation\n",
    " \n",
    " Q(s,a,parameters) = r + gamma*maxQ(s',a, parameters_offset)\n",
    " \n",
    " Cost function is mse of these two sides\n",
    " \n",
    " Cost =  (Q(s,a,parameters) - (r + gamma*maxQ(s',a, parameters_offset)))^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory Buffer\n",
    "A Deep Q learner is off policy, in that it will learn from a bunch of episodes completed on a policy that is not the current best policy.\n",
    "\n",
    "Using a replay buffer prevents it from forgetting valuable experience from other episodes and just adjusting overly to what it has just seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('state','action','next_state','reward'))\n",
    "\n",
    "class replay_memory():\n",
    "    \n",
    "    '''\n",
    "    class will store a bunch of past experiences\n",
    "    inputs \n",
    "        - size - size of memory\n",
    "    output\n",
    "        - object that stores (state, action, reward, next_state) tuples.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size =10000):\n",
    "        '''\n",
    "        empty list in which to store experiences\n",
    "        '''\n",
    "        self.storage = []\n",
    "        self.size = size\n",
    "        self.position = 0\n",
    "        \n",
    "        \n",
    "    def add_to_memory(self, *args):\n",
    "        '''\n",
    "        method will allow a new experience to be pushed into the memory buffer.\n",
    "        \n",
    "        inputs - experience- list [state, action, reward, next_state]\n",
    "        '''     \n",
    "        # if full memory full remove the first value in the list and then append the new one. \n",
    "        if len(self.storage) < self.size:\n",
    "            # i don't understand the append none part.\n",
    "            self.storage.append(None)\n",
    "        self.storage[self.position] = Transition(*args)\n",
    "        # make sure position is always within the size of the memory.\n",
    "        self.position = (self.position + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Get a random sample of the memory that can then be used as batch.\n",
    "        '''\n",
    "        return random.sample(self.storage, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepq_network(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    neural network to implement to deep q learner.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(deepq_network, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "          nn.Linear(8,512, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(512,256, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          # Output layer here is going to be a q value for each of the four actions.\n",
    "          nn.Linear(256, 4, bias=False),\n",
    "        )\n",
    "    \n",
    "    def Forward(self,input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action selection - as we use epsilon greedy some actions are random, others take the action that gives the max q value\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def action_selection(state):\n",
    "    global steps_done\n",
    "    #epsilon_greedy_approach\n",
    "    # get a random number between 0 and 1.\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end)* math.exp(-1*(steps_done/eps_decay))\n",
    "    steps_done += 1\n",
    "    if sample < epsilon:\n",
    "        # pick random action\n",
    "        action = torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
    "        return action\n",
    "    else:\n",
    "        # When you're just doing forward stuff you can drop the gradients and it saves memory.\n",
    "        with torch.no_grad():\n",
    "        # pick the action that the q_network thinks will give you the biggest q value. \n",
    "        # Make sure state is in the correct format.\n",
    "            state_tensor = torch.tensor([state]).to(dtype = torch.float)\n",
    "            action = Qnet.Forward(state_tensor).max(1)[1].view(1,1)  \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise():\n",
    "    '''\n",
    "    Function which after each step will run an optimisation of the q network.\n",
    "    \n",
    "    '''\n",
    "    # Before you can optimise the size of memory must to be full\n",
    "    if len(memory_storage) < batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory_storage.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    #print(batch)\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(dtype= torch.float)\n",
    "    \n",
    "    state_batch = torch.cat(batch.state).to(dtype= torch.float)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward).to(dtype= torch.float)\n",
    "    \n",
    "    # What's the predicted Q value for the current state\n",
    "    # Of these qvalues I need to pick the one at the index of the action we chose. \n",
    "    q_value = Qnet.Forward(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # What about the target_q. reward plus the max q value of the next state.\n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    \n",
    "    next_state_values[non_final_mask] = Tnet.Forward(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    target_q = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # What is the loss. The difference between these two. \n",
    "    # pytorch loss function\n",
    "    loss = mse_loss(q_value, target_q.unsqueeze(1))\n",
    "    \n",
    "    # Backprop the error in the q network.\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    # Clamp the gradient to avoid overly big changes and exploding gradients. \n",
    "    for param in Qnet.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimiser.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think about the training cycle.\n",
    "\n",
    "def training(episodes, learning_rate):\n",
    "    # Parameters\n",
    "    epsilon = 0.05\n",
    "    eps_start = 0.9\n",
    "    eps_end = 0.05\n",
    "    eps_decay = 200\n",
    "    copy_frequency = 10\n",
    "    batch_size = 128\n",
    "    GAMMA = 0.99\n",
    "    memory_storage = replay_memory(500000)\n",
    "\n",
    "    # For the double deep q network these are the two networks. \n",
    "    Qnet = deepq_network()\n",
    "    Tnet = deepq_network()\n",
    "\n",
    "    # Set the data type\n",
    "    Qnet=Qnet.float()\n",
    "    Tnet= Tnet.float()\n",
    "    Tnet.load_state_dict(Qnet.state_dict())\n",
    "\n",
    "    # you want tnet to be in evaluation mode rather than training mode. \n",
    "    # for example you wont have dropout.\n",
    "    Tnet.eval()\n",
    "\n",
    "    episode_durations = []\n",
    "\n",
    "    steps_done = 0 \n",
    "\n",
    "    #create an optimiser\n",
    "    optimiser = optim.Adam(Qnet.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    episodes = episodes\n",
    "    \n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        state = copy.deepcopy(env.reset())\n",
    "        for t in count():\n",
    "\n",
    "            # Decide which action to take. \n",
    "            action = action_selection(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "            next_state = list(next_state)\n",
    "\n",
    "            # Observe new state\n",
    "            if done:\n",
    "                next_state = None\n",
    "                next_state_save = None\n",
    "            else:\n",
    "                next_state_save = next_state[:]\n",
    "                next_state_save = torch.tensor([next_state_save])\n",
    "\n",
    "            reward = torch.tensor([reward]).to(dtype= torch.float)\n",
    "            state_save = state[:]\n",
    "            state_save = torch.tensor([state_save])\n",
    "\n",
    "            # save transition into the replay memory.\n",
    "            memory_storage.add_to_memory(state_save, action, next_state_save, reward)\n",
    "\n",
    "            # optimise the q network networks.\n",
    "            loss = optimise()\n",
    "\n",
    "            #check to see if the episode is done\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                rewards.append(reward.item())\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                #Move to the next state\n",
    "                state = next_state[:]   \n",
    "\n",
    "        # every so many episodes copy over the parameters of the qnet to the target net.\n",
    "        if episode % copy_frequency == 0:\n",
    "            Tnet.load_state_dict(Qnet.state_dict())\n",
    "        if episode % 50 ==0:\n",
    "            print('loss {} at epiode {}'.format(loss,episode))\n",
    "            print('reward {} at epiode {}'.format(reward,episode))\n",
    "    print('Finished')\n",
    "    # Close the environment.\n",
    "    env.close()\n",
    "    plt.plot(range(0,episodes), rewards)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    # plot the total episode reward against the number\n",
    "    # of episodes see whether we get \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to think about testing different hyperparameters:\n",
    "\n",
    "These are first ones i'd like to experiment with. \n",
    "- learning rate \n",
    "- discount factor\n",
    "- replay buffer size\n",
    "- batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 300\n",
    "learning_rate = 0.001\n",
    "training(episodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 300\n",
    "learning_rate = 0.0001\n",
    "training(episodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "-100\n",
      "Number of steps 62\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "-100\n",
      "Number of steps 90\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "-100\n",
      "Number of steps 70\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "-100\n",
      "Number of steps 89\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "-100\n",
      "Number of steps 59\n"
     ]
    }
   ],
   "source": [
    "max_steps = 300\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        \n",
    "        # Why's no grad not working..\n",
    "        #with torch.no_grad:\n",
    "        state_tensor = torch.tensor([state]).to(dtype= torch.float)\n",
    "        action = Qnet.Forward(state_tensor).max(1)[1].view(1, 1)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action.item())\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            print(reward)\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiements with different settings\n",
    "\n",
    "# A set up to allow automated testing of different settings.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
