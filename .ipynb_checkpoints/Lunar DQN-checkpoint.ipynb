{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from torch import optim\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "\n",
    "from itertools import count\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example action 2\n",
      "Example observation space [-0.00471716  1.4207451  -0.47780877  0.43665728  0.00547274  0.10823067\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Example action {}'.format(env.action_space.sample()))\n",
    "print('Example observation space {}'.format(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Environment\n",
    "\n",
    "The action space includes 4 discrete actions. (nothing, left burner, right burner, main burner )\n",
    "\n",
    "The first value controls the main thruster. \n",
    "The second value controls the left/right thruster.\n",
    "\n",
    "Observation space. 8 value vector is also continuous. \n",
    "\n",
    "What do the 8 values represent? Don't know but i guess it doesn't really matter. \n",
    "\n",
    "This is not a problem where you need to take the image and use that to work out where you are. It's more simple in that you have a vector that tells you the position. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00221558,  1.4143819 , -0.2244257 ,  0.15385094,  0.00257403,\n",
       "        0.05083577,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    #print('state {}'.format(state))\n",
    "    #print('reward {}'.format(type(reward)))\n",
    "    #print('done {}'.format(done))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Deep Q Learner From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reply memory \n",
    "\n",
    "# Q network\n",
    "\n",
    "# Target network\n",
    "\n",
    "# Training cycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory Buffer\n",
    "A Deep Q learner is off policy, in that it will learn from a bunch of episodes completed on a policy that is not the current best policy.\n",
    "\n",
    "Using a replay buffer prevents it from forgetting valuable experience from other episodes and just adjusting overly to what it has just seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('state','action','next_state','reward'))\n",
    "\n",
    "class replay_memory():\n",
    "    \n",
    "    '''\n",
    "    class will store a bunch of past experiences\n",
    "    inputs \n",
    "        - size - size of memory\n",
    "    output\n",
    "        - object that stores (state, action, reward, next_state) tuples.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size =1000):\n",
    "        '''\n",
    "        empty list in which to store experiences\n",
    "        '''\n",
    "        self.storage = []\n",
    "        self.size = size\n",
    "        self.position = 0\n",
    "        \n",
    "        \n",
    "    def add_to_memory(self, *args):\n",
    "        '''\n",
    "        method will allow a new experience to be pushed into the memory buffer.\n",
    "        \n",
    "        inputs - experience- list [state, action, reward, next_state]\n",
    "        '''     \n",
    "        # if full memory full remove the first value in the list and then append the new one. \n",
    "        if len(self.storage) < self.size:\n",
    "            # i don't understand the append none part.\n",
    "            self.storage.append(None)\n",
    "        self.storage[self.position] = Transition(*args)\n",
    "        # make sure position is always within the size of the memory.\n",
    "        self.position = (self.position + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Get a random sample of the memory that can then be used as batch.\n",
    "        '''\n",
    "        return random.sample(self.storage, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepq_network(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    neural network to implement to deep q learner.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(deepq_network, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "          nn.Linear(8,128, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(128,256, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(256,128, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(128,64, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(64,32, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          # Output layer here is going to be a q value for each of the four actions.\n",
    "          nn.Linear(32, 4, bias=False),\n",
    "          ## Need to get clear what the output of this should be.\n",
    "          nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def Forward(self,input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action selection - as we use epsilon greedy some actions are random, others take the action that gives the max q value\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def action_selection(state):\n",
    "    global steps_done\n",
    "    #epsilon_greedy_approach\n",
    "    # get a random number between 0 and 1.\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end)* math.exp(-1*(steps_done/eps_decay))\n",
    "    steps_done += 1\n",
    "    if sample < epsilon:\n",
    "        # pick random action\n",
    "        action = torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
    "        return action\n",
    "    else:\n",
    "        # I think we're doing this because we don't care about grads.So it's more \n",
    "        # memory efficient. \n",
    "        with torch.no_grad():\n",
    "        #pick the action that the q_network thinks will give you the biggest q value. \n",
    "        # MAKE SURE STATE IS IN THE CORRECT FORMAT. \n",
    "            state_tensor = torch.tensor([state]).to(dtype = torch.float)\n",
    "            action = Qnet.Forward(state_tensor).max(1)[1].view(1,1)  \n",
    "        return action\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_storage = replay_memory()\n",
    "Qnet = deepq_network()\n",
    "Tnet = deepq_network()\n",
    "# Set the data type\n",
    "Qnet=Qnet.float()\n",
    "Tnet= Tnet.float()\n",
    "Tnet.load_state_dict(Qnet.state_dict())\n",
    "# you want tnet to be in evaluation mode rather than training mode. \n",
    "# for example you wont have dropout. \n",
    "Tnet.eval()\n",
    "\n",
    "epsilon = 0.05\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 200\n",
    "copy_frequency = 10\n",
    "batch_size = 64\n",
    "GAMMA = 0.999\n",
    "#\n",
    "episode_durations = []\n",
    "\n",
    "steps_done = 0 \n",
    "\n",
    "#create an optimiser\n",
    "optimiser = optim.Adam(Qnet.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise():\n",
    "    '''\n",
    "    Function which after each step will run an optimisation of the q network.\n",
    "    \n",
    "    '''\n",
    "    # Before you can optimise the size of memory must to be full\n",
    "    if len(memory_storage) < batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory_storage.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    #print(batch)\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(dtype= torch.float)\n",
    "    \n",
    "    state_batch = torch.cat(batch.state).to(dtype= torch.float)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward).to(dtype= torch.float)\n",
    "    \n",
    "    # What's the predicted Q value for the current state\n",
    "    # Of these qvalues i need to pick the one at the index of the action we chose. \n",
    "    q_value = Qnet.Forward(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # What about the target_q. reward plus the max q value of the next state.\n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    \n",
    "    next_state_values[non_final_mask] = Tnet.Forward(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    target_q = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # What is the loss. The difference between these two. \n",
    "    # pytorch loss function\n",
    "    loss = mse_loss(q_value, target_q.unsqueeze(1))\n",
    "    \n",
    "    # Backprop the error in the q network.\n",
    "    # How do these three link everything together? \n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in Qnet.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimiser.step()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 168.3443603515625 at epiode 0\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Think about the training cycle.\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = copy.deepcopy(env.reset())\n",
    "    for t in count():\n",
    "        \n",
    "        # Decide which action to take. \n",
    "        action = action_selection(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        next_state = list(next_state)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "            next_state_save = None\n",
    "        else:\n",
    "            next_state_save = next_state[:]\n",
    "            next_state_save = torch.tensor([next_state_save])\n",
    "\n",
    "        reward = torch.tensor([reward]).to(dtype= torch.float)\n",
    "        state_save = state[:]\n",
    "        state_save = torch.tensor([state_save])\n",
    "        \n",
    "        # save transition into the replay memory.\n",
    "        memory_storage.add_to_memory(state_save, action, next_state_save, reward)\n",
    "        \n",
    "        # optimise the q network networks.\n",
    "        loss = optimise()\n",
    "\n",
    "        #check to see if the episode is done\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "        else:\n",
    "            #Move to the next state\n",
    "            state = next_state[:]   \n",
    "            \n",
    "    # every so many episodes copy over the parameters of the qnet to the target net.\n",
    "    if episode % copy_frequency == 0:\n",
    "        Tnet.load_state_dict(Qnet.state_dict())\n",
    "    if episode % 1000 == 0 :\n",
    "        print('loss {} at epiode {}'.format(loss,episode))\n",
    "    \n",
    "print('Finished')\n",
    "# Close the environment.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "-100\n",
      "Number of steps 96\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "-100\n",
      "Number of steps 77\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "-100\n",
      "Number of steps 74\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "-100\n",
      "Number of steps 102\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "-100\n",
      "Number of steps 76\n"
     ]
    }
   ],
   "source": [
    "max_steps = 300\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        \n",
    "        state_tensor = torch.tensor([state]).to(dtype= torch.float)\n",
    "        action = Qnet.Forward(state_tensor).max(1)[1].view(1, 1)\n",
    "        new_state, reward, done, info = env.step(action.item())\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            print(reward)\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
