{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters to explore\n",
    "\n",
    "epochs to train\n",
    "\n",
    "shape of the network \n",
    "     - number of layers\n",
    "     - no softmax at the end\n",
    "\n",
    "learning rate \n",
    "\n",
    "discount factor\n",
    "\n",
    "replay buffer size\n",
    "\n",
    "batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "from torch import optim\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "\n",
    "from itertools import count\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example action 1\n",
      "Example observation space [ 0.00613947  1.4179344   0.6218396   0.31173033 -0.00710724 -0.14085582\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Example action {}'.format(env.action_space.sample()))\n",
    "print('Example observation space {}'.format(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Environment\n",
    "\n",
    "The action space includes 4 discrete actions. (nothing, left burner, right burner, main burner )\n",
    "\n",
    "The first value controls the main thruster. \n",
    "The second value controls the left/right thruster.\n",
    "\n",
    "Observation space. 8 value vector is also continuous. \n",
    "\n",
    "What do the 8 values represent? Don't know but i guess it doesn't really matter. \n",
    "\n",
    "This is not a problem where you need to take the image and use that to work out where you are. It's more simple in that you have a vector that tells you the position. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00149927,  1.4203099 ,  0.15185699,  0.4173237 , -0.00173062,\n",
       "       -0.03439791,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    #print('state {}'.format(state))\n",
    "    #print('reward {}'.format(type(reward)))\n",
    "    #print('done {}'.format(done))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Deep Q Learner From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reply memory \n",
    "\n",
    "# Q network\n",
    "\n",
    "# Target network\n",
    "\n",
    "# Training cycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory Buffer\n",
    "A Deep Q learner is off policy, in that it will learn from a bunch of episodes completed on a policy that is not the current best policy.\n",
    "\n",
    "Using a replay buffer prevents it from forgetting valuable experience from other episodes and just adjusting overly to what it has just seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('state','action','next_state','reward'))\n",
    "\n",
    "class replay_memory():\n",
    "    \n",
    "    '''\n",
    "    class will store a bunch of past experiences\n",
    "    inputs \n",
    "        - size - size of memory\n",
    "    output\n",
    "        - object that stores (state, action, reward, next_state) tuples.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size =10000):\n",
    "        '''\n",
    "        empty list in which to store experiences\n",
    "        '''\n",
    "        self.storage = []\n",
    "        self.size = size\n",
    "        self.position = 0\n",
    "        \n",
    "        \n",
    "    def add_to_memory(self, *args):\n",
    "        '''\n",
    "        method will allow a new experience to be pushed into the memory buffer.\n",
    "        \n",
    "        inputs - experience- list [state, action, reward, next_state]\n",
    "        '''     \n",
    "        # if full memory full remove the first value in the list and then append the new one. \n",
    "        if len(self.storage) < self.size:\n",
    "            # i don't understand the append none part.\n",
    "            self.storage.append(None)\n",
    "        self.storage[self.position] = Transition(*args)\n",
    "        # make sure position is always within the size of the memory.\n",
    "        self.position = (self.position + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Get a random sample of the memory that can then be used as batch.\n",
    "        '''\n",
    "        return random.sample(self.storage, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepq_network(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    neural network to implement to deep q learner.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(deepq_network, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "          nn.Linear(8,128, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(128,64, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(64,32, bias=False),\n",
    "          nn.ReLU(True),\n",
    "          # Output layer here is going to be a q value for each of the four actions.\n",
    "          nn.Linear(32, 4, bias=False),\n",
    "          ## Need to get clear what the output of this should be.\n",
    "        )\n",
    "    \n",
    "    def Forward(self,input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action selection - as we use epsilon greedy some actions are random, others take the action that gives the max q value\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def action_selection(state):\n",
    "    global steps_done\n",
    "    #epsilon_greedy_approach\n",
    "    # get a random number between 0 and 1.\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end)* math.exp(-1*(steps_done/eps_decay))\n",
    "    steps_done += 1\n",
    "    if sample < epsilon:\n",
    "        # pick random action\n",
    "        action = torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
    "        return action\n",
    "    else:\n",
    "        # I think we're doing this because we don't care about grads.So it's more \n",
    "        # memory efficient. \n",
    "        with torch.no_grad():\n",
    "        #pick the action that the q_network thinks will give you the biggest q value. \n",
    "        # MAKE SURE STATE IS IN THE CORRECT FORMAT. \n",
    "            state_tensor = torch.tensor([state]).to(dtype = torch.float)\n",
    "            action = Qnet.Forward(state_tensor).max(1)[1].view(1,1)  \n",
    "        return action\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_storage = replay_memory()\n",
    "Qnet = deepq_network()\n",
    "Tnet = deepq_network()\n",
    "# Set the data type\n",
    "Qnet=Qnet.float()\n",
    "Tnet= Tnet.float()\n",
    "Tnet.load_state_dict(Qnet.state_dict())\n",
    "# you want tnet to be in evaluation mode rather than training mode. \n",
    "# for example you wont have dropout. \n",
    "Tnet.eval()\n",
    "\n",
    "epsilon = 0.05\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 200\n",
    "copy_frequency = 10\n",
    "batch_size = 128\n",
    "GAMMA = 0.999\n",
    "#\n",
    "episode_durations = []\n",
    "\n",
    "steps_done = 0 \n",
    "\n",
    "#create an optimiser\n",
    "optimiser = optim.Adam(Qnet.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise():\n",
    "    '''\n",
    "    Function which after each step will run an optimisation of the q network.\n",
    "    \n",
    "    '''\n",
    "    # Before you can optimise the size of memory must to be full\n",
    "    if len(memory_storage) < batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory_storage.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    #print(batch)\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(dtype= torch.float)\n",
    "    \n",
    "    state_batch = torch.cat(batch.state).to(dtype= torch.float)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward).to(dtype= torch.float)\n",
    "    \n",
    "    # What's the predicted Q value for the current state\n",
    "    # Of these qvalues i need to pick the one at the index of the action we chose. \n",
    "    q_value = Qnet.Forward(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # What about the target_q. reward plus the max q value of the next state.\n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    \n",
    "    next_state_values[non_final_mask] = Tnet.Forward(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    target_q = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # What is the loss. The difference between these two. \n",
    "    # pytorch loss function\n",
    "    loss = mse_loss(q_value, target_q.unsqueeze(1))\n",
    "    \n",
    "    # Backprop the error in the q network.\n",
    "    # How do these three link everything together? \n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in Qnet.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimiser.step()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss None at epiode 0\n",
      "loss 1.3296540975570679 at epiode 50\n",
      "loss 0.596761167049408 at epiode 100\n",
      "loss 1.6157256364822388 at epiode 150\n",
      "loss 58.45109558105469 at epiode 200\n",
      "loss 15.241861343383789 at epiode 250\n",
      "loss 7.68552827835083 at epiode 300\n",
      "loss 14.623601913452148 at epiode 350\n",
      "loss 16.778400421142578 at epiode 400\n",
      "loss 21.164575576782227 at epiode 450\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Think about the training cycle.\n",
    "episodes = 500\n",
    "rewards = []\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = copy.deepcopy(env.reset())\n",
    "    for t in count():\n",
    "        \n",
    "        # Decide which action to take. \n",
    "        action = action_selection(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        next_state = list(next_state)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "            next_state_save = None\n",
    "        else:\n",
    "            next_state_save = next_state[:]\n",
    "            next_state_save = torch.tensor([next_state_save])\n",
    "\n",
    "        reward = torch.tensor([reward]).to(dtype= torch.float)\n",
    "        state_save = state[:]\n",
    "        state_save = torch.tensor([state_save])\n",
    "        \n",
    "        # save transition into the replay memory.\n",
    "        memory_storage.add_to_memory(state_save, action, next_state_save, reward)\n",
    "        \n",
    "        # optimise the q network networks.\n",
    "        loss = optimise()\n",
    "\n",
    "        #check to see if the episode is done\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            rewards.append(reward.item())\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            #Move to the next state\n",
    "            state = next_state[:]   \n",
    "            \n",
    "    # every so many episodes copy over the parameters of the qnet to the target net.\n",
    "    if episode % copy_frequency == 0:\n",
    "        Tnet.load_state_dict(Qnet.state_dict())\n",
    "    if episode % 50 ==0:\n",
    "        print('loss {} at epiode {}'.format(loss,episode))\n",
    "    \n",
    "print('Finished')\n",
    "# Close the environment.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c0f7a569b0>]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes \n",
    "\n",
    "plt.plot(range(0,episodes), rewards)\n",
    "# plot the total episode reward against the number\n",
    "# of episodes see whether we get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "-100\n",
      "Number of steps 102\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "-100\n",
      "Number of steps 145\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "-100\n",
      "Number of steps 137\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "-100\n",
      "Number of steps 61\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "-100\n",
      "Number of steps 146\n"
     ]
    }
   ],
   "source": [
    "max_steps = 300\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        with torch.no_grad:\n",
    "            state_tensor = torch.tensor([state]).to(dtype= torch.float)\n",
    "            action = Qnet.Forward(state_tensor).max(1)[1].view(1, 1)\n",
    "        new_state, reward, done, info = env.step(action.item())\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            print(reward)\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
